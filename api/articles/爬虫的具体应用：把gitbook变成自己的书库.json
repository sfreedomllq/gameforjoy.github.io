{"title":"爬虫的具体应用：把gitbook变成自己的书库","slug":"爬虫的具体应用：把gitbook变成自己的书库","date":"2018-12-03T01:15:49.000Z","updated":"2018-12-04T12:15:28.470Z","comments":true,"path":"api/articles/爬虫的具体应用：把gitbook变成自己的书库.json","photos":[],"link":"","excerpt":"把gitbook变成自己的书库这篇文章中我们会从针对 gitbook 编写一个爬虫程序，该程序可以从 gitbook 网站中爬取我们想要的书并保存为pdf文件。主要应用到的技术点包括 beautifulsoup+requests， PyPDF2 。<br>为什么不用 scrapy？ 首先我们这种量级的爬虫程序用 scrapy 有点大材小用了。 其次，用 beautifulsoup+requests 可以帮助我们理解工作逻辑， 以及一些紧急措施的制定实行， 这在程序的编写中尤其重要。思路&amp;准备思路很简单： beautifulSoup+requests 实现对需求数据的爬取， 然后进行一定的数据处理保存为html文件，再通过 PyPDF2 将保存好的html文件转换成pdf文件。<br>准备：安装 requests、 beautifulsoup、 pdfkit，reuqests 用于网络请求，beautifusoup 用于操作html数据， pdfkit 是 wkhtmltopdf 的Python封装包；安装 wkhtmltopdf，wkhtmltopdf 是一个适用于多平台的实现html对pdf转换的工具；安装 PyPDF2，此工具用于对pdf的融合。","covers":null,"content":"<h1 id=\"把gitbook变成自己的书库\"><a href=\"#把gitbook变成自己的书库\" class=\"headerlink\" title=\"把gitbook变成自己的书库\"></a>把gitbook变成自己的书库</h1><p>这篇文章中我们会从针对 gitbook 编写一个爬虫程序，该程序可以从 gitbook 网站中爬取我们想要的书并保存为pdf文件。主要应用到的技术点包括 beautifulsoup+requests， PyPDF2 。<br>为什么不用 scrapy？ 首先我们这种量级的爬虫程序用 scrapy 有点大材小用了。 其次，用 beautifulsoup+requests 可以帮助我们理解工作逻辑， 以及一些紧急措施的制定实行， 这在程序的编写中尤其重要。</p>\n<h2 id=\"思路-amp-准备\"><a href=\"#思路-amp-准备\" class=\"headerlink\" title=\"思路&amp;准备\"></a>思路&amp;准备</h2><p>思路很简单： beautifulSoup+requests 实现对需求数据的爬取， 然后进行一定的数据处理保存为html文件，再通过 PyPDF2 将保存好的html文件转换成pdf文件。<br><strong>准备：</strong></p>\n<ol>\n<li>安装 requests、 beautifulsoup、 pdfkit，reuqests 用于网络请求，beautifusoup 用于操作html数据， pdfkit 是 wkhtmltopdf 的Python封装包；</li>\n<li>安装 wkhtmltopdf，wkhtmltopdf 是一个适用于多平台的实现html对pdf转换的工具；</li>\n<li>安装 PyPDF2，此工具用于对pdf的融合。</li>\n</ol>\n<a id=\"more\"></a>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"PLAIN\"><figure class=\"iseeu highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def get_chapter(bookname,url):</span><br><span class=\"line\">    </span><br><span class=\"line\">    i = 0     </span><br><span class=\"line\"></span><br><span class=\"line\">    //用来存放所有章节生成的pdf文件名                 </span><br><span class=\"line\">    pdfs = []   </span><br><span class=\"line\"></span><br><span class=\"line\">    //设定存放爬取到的页面数据的html文件模板                        </span><br><span class=\"line\">    html_template = &quot;&quot;&quot;                 </span><br><span class=\"line\">        &lt;!DOCTYPE html&gt;</span><br><span class=\"line\">        &lt;html lang=&quot;en&quot;&gt;</span><br><span class=\"line\">        &lt;head&gt;</span><br><span class=\"line\">            &lt;meta charset=&quot;UTF-8&quot;&gt;</span><br><span class=\"line\">        &lt;/head&gt;</span><br><span class=\"line\">        &lt;body&gt;</span><br><span class=\"line\">        &#123;content&#125;</span><br><span class=\"line\">        &lt;/body&gt;</span><br><span class=\"line\">        &lt;/html&gt;</span><br><span class=\"line\">        &quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">    //实例化一个soup</span><br><span class=\"line\">    soup = BeautifulSoup(requests.get(url).text, &apos;html.parser&apos;)     </span><br><span class=\"line\">    </span><br><span class=\"line\">    //一个循环遍历所有的章节信息</span><br><span class=\"line\">    for cursor in soup.find_all( class_=&apos;chapter&apos; ):        </span><br><span class=\"line\">        i = i+1</span><br><span class=\"line\">        print(cursor.text)</span><br><span class=\"line\"></span><br><span class=\"line\">        //拼接保存数据的html文件和pdf文件的文件名</span><br><span class=\"line\">        pdf_name = &apos;docker&apos;+ str(i) + &apos;.pdf&apos;</span><br><span class=\"line\">        html_name = &apos;docker&apos;+ str(i) + &apos;.html&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">        //拼接章节详细页的链接</span><br><span class=\"line\">        link = url+cursor.a.get(&apos;href&apos;)    </span><br><span class=\"line\"></span><br><span class=\"line\">        //强制进程等待两秒再执行，anti-反爬虫策略 </span><br><span class=\"line\">        time.sleep(2);      </span><br><span class=\"line\"></span><br><span class=\"line\">        //或取详细内容页信息                </span><br><span class=\"line\">        html = get_chapter_text(link)    </span><br><span class=\"line\"></span><br><span class=\"line\">        //将爬取到的信息插入到设定好的html文件模板中</span><br><span class=\"line\">        html = html_template.format(content = html)</span><br><span class=\"line\">        html = html.encode(&quot;utf-8&quot;)</span><br><span class=\"line\">        </span><br><span class=\"line\">        //将组织好的html信息写如html文件，再将html文件写入pdf文件，然后将pdf文件名添加到pdfs[]中</span><br><span class=\"line\">        with open( html_name, &apos;wb&apos;) as f:</span><br><span class=\"line\">            f.write(html)</span><br><span class=\"line\">        save_pdf(html_name,pdf_name )</span><br><span class=\"line\">        pdfs.append(pdf_name)</span><br><span class=\"line\">    </span><br><span class=\"line\">    //调用merge_pdf()将所有的pdf文件融合成 bookname.pdf</span><br><span class=\"line\">    merge_pdf(bookname, pdfs)</span><br><span class=\"line\"></span><br><span class=\"line\">def merge_pdf(bookname,pdfs):</span><br><span class=\"line\">    merger = PdfFileMerger()</span><br><span class=\"line\">    i = 0</span><br><span class=\"line\">    for pdf in pdfs:</span><br><span class=\"line\">        merger.append(open(pdf, &apos;rb&apos;), import_bookmarks=False)</span><br><span class=\"line\">        print( u&quot;合并完成第&quot; + str(i) + &apos;个pdf&apos; + pdf)</span><br><span class=\"line\">        i = i + 1</span><br><span class=\"line\">    output = open(bookname+&quot;.pdf&quot;, &quot;wb&quot;)</span><br><span class=\"line\">    merger.write(output)</span><br><span class=\"line\">    print(u&quot;输出PDF成功！&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">def get_chapter_text(url):</span><br><span class=\"line\">    current_soup = BeautifulSoup(requests.get(url).text, &apos;html.parser&apos;)</span><br><span class=\"line\">    content = current_soup.find(class_=&apos;search-noresults&apos;)</span><br><span class=\"line\">    return(content)</span><br><span class=\"line\"></span><br><span class=\"line\">def save_pdf(htmls, file_name):</span><br><span class=\"line\">  &quot;&quot;&quot;</span><br><span class=\"line\">  把所有html文件保存到pdf文件</span><br><span class=\"line\">  :param htmls: html文件列表</span><br><span class=\"line\">  :param file_name: pdf文件名</span><br><span class=\"line\">  :return:</span><br><span class=\"line\">  &quot;&quot;&quot;</span><br><span class=\"line\">  options = &#123;</span><br><span class=\"line\">    &apos;page-size&apos;: &apos;Letter&apos;,</span><br><span class=\"line\">    &apos;margin-top&apos;: &apos;0.75in&apos;,</span><br><span class=\"line\">    &apos;margin-right&apos;: &apos;0.75in&apos;,</span><br><span class=\"line\">    &apos;margin-bottom&apos;: &apos;0.75in&apos;,</span><br><span class=\"line\">    &apos;margin-left&apos;: &apos;0.75in&apos;,</span><br><span class=\"line\">    &apos;encoding&apos;: &quot;UTF-8&quot;,</span><br><span class=\"line\">    &apos;custom-header&apos;: [</span><br><span class=\"line\">      (&apos;Accept-Encoding&apos;, &apos;gzip&apos;)</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &apos;cookie&apos;: [</span><br><span class=\"line\">      (&apos;cookie-name1&apos;, &apos;cookie-value1&apos;),</span><br><span class=\"line\">      (&apos;cookie-name2&apos;, &apos;cookie-value2&apos;),</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &apos;outline-depth&apos;: 10,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  pdfkit.from_file(htmls, file_name, options=options)</span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    url = &apos;https://yeasy.gitbooks.io/docker_practice/content/&apos;</span><br><span class=\"line\">    get_chapter(&quot;Docker_all&quot;,url)</span><br></pre></td></tr></table></figure></div>\n<p>核心代码已经贴出，并在代码中给了相应步骤的注释。这里再做一点细节上的补充：</p>\n<ol>\n<li>beautifulsoup 通过对需求数据所在的元素定位进行爬取，所以我们要学会看网页源码查看我们需要的数据元素，具体可用定位方法请参阅文档；</li>\n<li><p><code>merger.append(open(pdf, &#39;rb&#39;), import_bookmarks=False)</code> 这行代码中的 <code>import_bookmarks=False</code> 并非必须，但因为PyPDF2中的某些问题需要添加此参数已防止程序报错 <code>PyPDF2.utils.PdfReadError: Unexpected destination &#39;/__WKANCHOR_2&#39;</code> ；或者在 pdf.py 文件中 1200——1500 行左右的代码，将 return 前的一个else注释掉也可避免此错误：</p>\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"PLAIN\"><figure class=\"iseeu highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># if destination found, then create outline</span><br><span class=\"line\">   if dest:</span><br><span class=\"line\">       if isinstance(dest, ArrayObject):</span><br><span class=\"line\">           outline = self._buildDestination(title, dest)</span><br><span class=\"line\">       elif isinstance(dest, Str) and dest in self._namedDests:</span><br><span class=\"line\">           outline = self._namedDests[dest]</span><br><span class=\"line\">           outline[NameObject(&quot;/Title&quot;)] = title</span><br><span class=\"line\">       # else:</span><br><span class=\"line\">       #     raise utils.PdfReadError(&quot;Unexpected destination %r&quot; % dest)</span><br><span class=\"line\">   return outline</span><br></pre></td></tr></table></figure></div>\n</li>\n<li><p>save_pdf() 方法需要两个参数， 一个是待转换的html文件，另一个是转换完成的pdf文件名，也就是说操作完成时每一章节会有两个对应文件：一个 XX.html 和 一个 XX.pdf，这点在代码运行的时候就能看到。在数据全部爬取完成之后删除即可，也可在 merge_pdf() 执行完成后添加一步删除操作（毕竟merge完之后也用不到了，太多东西看着烦）。</p>\n</li>\n</ol>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>看起来似乎没什么东西，但想想看，这么一个脚本，只要给一个gitbook上的任意一本书的url，就能以自定义的书名将该书保存到本地，好像还蛮酷的啊~<br>刚完成代码进行测试的时候，发现基本每次爬到70——80章的时候就会报错。 两到三次之后，我意识到可能是gitbook上设置的反爬虫策略，于是做了一点简单的处理：添加了 <code>time.sleep(2)</code> 这么一行代码， 每一篇文章内容在爬取之前都会等待2秒钟，相当于 scrapy中 的 download_delay ，再运行的时候就OK了。我讲这类处理称为： anti-反爬虫策略。<br>常见的 anti-反爬虫策略 大致分为5种 ：</p>\n<ol>\n<li>设置下载等待时间，其实就是降低了对服务器的访问频率，就如同本文中的 time.sleep() 操作；</li>\n<li>设置 User-Agent 池。User-Agent是指包含浏览器信息、操作系统信息等的一个字符串，也称之为一种特殊的网络协议。有些服务器通过它判断当前访问对象是浏览器、邮件客户端还是网络爬虫。所以我们可以通过修改 User-Agent 来伪装成不同浏览器来欺骗服务器；</li>\n<li>设置IP池。通过不断更换IP对服务器进行欺骗，但要注意这些IP不要是同一IP段的，有的网站封的是IP段，那么同段的IP再多也没用；</li>\n<li>禁止cookies。针对一些会通过cookies来识别爬虫轨迹的网站；</li>\n<li>分布式爬取。这条点到为止，别问我为啥….</li>\n</ol>\n<p>资料查阅到这里，我又有了一个新的疑问：如何判定一个网站采用的是哪种反爬虫策略？毕竟知己知彼，我们在编写程序的时候才好设定对策。这点等我研究明白了再来更新。</p>\n<p>此脚本已经公布在 <a href=\"https://github.com/sfreedomllq/get_gitbook.git\" target=\"_blank\" rel=\"noopener\">https://github.com/sfreedomllq/get_gitbook.git</a> 上,其实对于输出的pdf文件的内的排版格式还有待改进，所以有兴趣的朋友不妨 fork 下来试试~</p>\n","categories":[{"name":"实践","slug":"实践","count":1,"path":"api/categories/实践.json"}],"tags":[{"name":"beautifulsoup","slug":"beautifulsoup","count":1,"path":"api/tags/beautifulsoup.json"},{"name":"requests","slug":"requests","count":1,"path":"api/tags/requests.json"},{"name":"PyPDF2","slug":"PyPDF2","count":1,"path":"api/tags/PyPDF2.json"},{"name":"gitbook","slug":"gitbook","count":1,"path":"api/tags/gitbook.json"},{"name":"爬虫","slug":"爬虫","count":2,"path":"api/tags/爬虫.json"}]}